# -*- coding: utf-8 -*-
"""marketing_campaign.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15MmyTgaCuDMXdQgkgS0iaFgo5QBay_s8

# Assignment: Marketing Campaign Data Analysis

**Welcome, aspiring data analysts!** In this assignment, you will work with a real-world marketing campaign dataset. Your goal is to clean, transform, explore, and engineer new features to prepare the data for analysis and, eventually, for machine learning models.

Follow the steps in this guide to make the data ready for insightful analysis.

---

### Initial Setup
First, let's import the necessary libraries for our analysis.
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Set plotting style
sns.set(style='whitegrid')

"""---

## ðŸ“š Part 1: Data Cleaning & Exploratory Data Analysis (EDA)

**Goal:** To understand the raw dataset, identify any initial problems, and perform basic cleaning.

### 1. Load and Explore the Data

**Task:** Load the `marketing_campaign.csv` file into a Pandas DataFrame. Remember that the separator in this file is a tab (`\t`).
"""

# TASK: Load the dataset
df = pd.read_csv('marketing_campaign.csv' , sep='\t')

"""**Task:** Use the `.head()`, `.info()`, `.describe()`, and `.shape` functions to get a first look at your data."""

# Get the first 5 rows
df.head(5)

# Get a summary of the DataFrame
df.info()

# Get statistical summary for numerical columns
df.describe()

# Get the dimensions of the DataFrame (rows, columns)
df.shape

"""**Question:** Based on these initial outputs, what are your first observations about the data types, number of entries, and potential missing values?

**Your Answer Here:**

*Type your observations here. For example:*


*   Income column has missing values.

*   2240 entries

*   columns(Education , Merital_Status , Dt_Customer) have "objects" data type.
"""



"""### 2. Data Cleansing

You will notice two columns, `Z_CostContact` and `Z_Revenue`, that have the same value for every single row.

**Explain:** Why are columns with a single, constant value not useful for building machine learning models?

**Your Answer Here:**

*Type your explanation here. Think about what a model tries to learn. If a feature doesn't change, what can the model learn from it?*

1) Inaccurate prediction.
2) difficult to use the algorithm as if the algorithm couldn't ignore the redundancy automatically it will be noisy in the model.

**Task:** Remove these two columns from your DataFrame.
"""

# TASK: Drop the unnecessary columns
df.drop(columns=['Z_CostContact', 'Z_Revenue'], inplace=True)

"""---

## ðŸ“ Part 2: Handling Missing Data

**Goal:** To properly address empty (`NaN`) values to ensure data quality.

### 1. Identify Missing Values

**Task:** Use the `.isna().sum()` function to identify which columns contain missing values and how many.
"""

# TASK: Find the sum of missing values for each column
df.isna().sum()

"""### 2. Imputation Strategy

**Task:** Fill the missing values in the `Income` column using the column's **mean**.
"""

# TASK: Fill missing income values with the mean
df['Income'].fillna(df['Income'].mean(), inplace=True)

# Verify that there are no more missing values

df.isna().sum()

"""**Question:** Why might using the mean be a suitable choice here? In what situations might using the **median** be a better choice?

**Your Answer Here:**

*Type your explanation here. Consider the effect of outliers on the mean and median.*

Because there is no extreme outliers.
The income columns is a continuous variable.

---

## ðŸ› ï¸ Part 3: Feature Engineering & Transformation

**Goal:** To create new, more meaningful features from existing ones to improve model performance.

### 1. Create New Features

**Task:** Create the new features as described below.
- Create New Features: Create the following new columns in your DataFrame:

- Age: Calculate the customer's age from the Year_Birth column. (You can use 2025 as the current year for calculation).

- Kids: Create a feature for the total number of children in the household by summing Kidhome and Teenhome.

- Expenses: Create a column representing the total amount spent by summing all columns that start with Mnt... (e.g., MntWines, MntFruits, etc.).

- TotalAcceptedCmp: Create a column that counts the total number of campaigns a customer responded to. Sum the columns from AcceptedCmp1 to AcceptedCmp5 and the Response column.

- NumTotalPurchases: Create a column for the total number of purchases made. Sum the NumWebPurchases, NumCatalogPurchases, NumStorePurchases, and NumDealsPurchases columns.

- day_engaged: Create a feature that shows how many days a customer has been with the company. (Hint: You will need to convert the Dt_Customer column to a datetime format and then calculate the difference between that date and a recent date, like '2025-01-01').
"""

# TASK: Create 'Age' column. We'll use 2025 as the reference year for consistency.
df['Age'] = 2025 - df['Year_Birth']

# TASK: Create 'Kids' column
df['Kids'] = df['Kidhome'] + df['Teenhome']

# TASK: Create 'Expenses' column
df['Expenses'] = df['MntWines'] + df['MntFruits'] + df['MntMeatProducts'] + df['MntFishProducts']

# TASK: Create 'TotalAcceptedCmp' column
df['TotalAcceptedCmp'] = df['AcceptedCmp1'] + df['AcceptedCmp2'] + df['AcceptedCmp3'] + df['AcceptedCmp4']

# TASK: Create 'NumTotalPurchases' column
df['NumTotalPurchases'] = df['NumWebPurchases'] + df['NumCatalogPurchases'] + df['NumStorePurchases'] + df['NumDealsPurchases']

# TASK: Create 'day_engaged' column
# First, convert 'Dt_Customer' to datetime objects
from datetime import datetime
df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], format='%d-%m-%Y')
# Then, calculate the days engaged relative to a fixed recent date ('2025-01-01')
df['day_engaged'] = (datetime(2025, 11, 12) - df['Dt_Customer']).dt.days

"""### 2. Remove Redundant Columns

**Task:** Now that we have our new features, let's drop the old ones to avoid data duplication and noise.
"""

# TASK: Create a list of columns to delete
cols_to_drop = [
    'Dt_Customer'
]

# Drop the columns
df.drop(columns=cols_to_drop)
# Display the first few rows of the cleaned dataframe
display(df.head())

"""---

## ðŸ“Š Part 4: Handling Categorical Data

**Goal:** To convert text-based data into a numerical format that models can understand.

### 1. Simplify Categories (Reducing Cardinality)

**Task:** Use `.value_counts()` to see the categories in `Marital_Status` and `Education`.
"""

df['Education'].value_counts()

df['Marital_Status'].value_counts()

"""**Task:** Combine the categories as described in the assignment instructions.

- Combine 'Married' and 'Together' into a single category called 'relationship'.

- Combine 'Divorced', 'Widow', 'Alone', 'YOLO', and 'Absurd' into a single category called 'Single'.

- Task: Do the same for the Education column. For example, combine 'PhD', 'Master', '2n Cycle', and 'Graduation' into one category like 'Postgraduate' and 'Basic' into 'Undergraduate'.
"""

# TASK: Simplify 'Marital_Status'
df['relationship'] = df['Marital_Status'].replace(['Married' , 'Together'] , 'relationship')
df['single'] = df['Marital_Status'].replace(['Divorced' , 'Widow' , 'Alone' , 'YOLO' , 'Absurd'] , 'Single')

# TASK: Simplify 'Education'
df['graduation'] = df['Education'].replace(['PhD' , 'Master' , '2n Cycle' , 'Graduation'] , 'graduation')
df['basic'] = df['Education'].replace(['Basic'] , 'Ungraduate')

"""### 2. Categorical Encoding

**Explain:** In your own words, what is the difference between Label Encoding and One-Hot Encoding?
"""



"""**Your Answer Here:**

*Type your explanation here. Think about what the output looks like for each method and if the resulting numbers have any order.*

Label Encoder:
assign unique integers to the categories.

Hot Encoder:
assign binary(0/1)columns for the categories

**Task:** Use `LabelEncoder` to convert the simplified `Education` and `Marital_Status` columns into numbers.
"""

from sklearn.preprocessing import LabelEncoder

# TASK: Apply LabelEncoder to the categorical columns
label_encoder = LabelEncoder()
df['Education'] = label_encoder.fit_transform(df['Education'])
df['Marital_Status'] = label_encoder.fit_transform(df['Marital_Status'])

# Display the head to see the changes
display(df.head())

"""---

## ðŸ“ˆ Part 5: Detecting and Handling Outliers

**Goal:** To identify and manage extreme values (outliers) that could negatively affect model performance.

### 1. Detecting Outliers

A simple way to visualize outliers is with a box plot. Let's look at `Income` and `Age`.
"""

plt.figure(figsize=(10, 4))
sns.boxplot(x=df['Income'])
plt.title('Box Plot of Income')
plt.show()

plt.figure(figsize=(10, 4))
sns.boxplot(x=df['Age'])
plt.title('Box Plot of Age')
plt.show()

"""**Question:** Why can outliers be a problem in data analysis and machine learning?

**Your Answer Here:**

*Type your explanation here. Think about how a very large or small value might affect calculations like the mean.*

It may cause inaccurate calculations (mean).
It could cause difficulties in choosing the algorithm.
It may affect the model while building so thw prediction could be inaccurate.

### 2. Handling Outliers

**Task:** Remove the rows that contain these outliers. A common method is to remove values that are outside of 1.5 times the Interquartile Range (IQR). We will remove outliers from `Income` and `Age`.
"""

print(f"Original shape: {df.shape}")

# TASK: Remove outliers based on the IQR method
for column in ['Income', 'Age']:
    Q1, Q3 = df[column].quantile([0.25, 0.75])
    IQR = Q3 - Q1
    df = df[(df[column] >= Q1 - 1.5*IQR) & (df[column] <= Q3 + 1.5*IQR)]

print(f"Shape after removing outliers: {df.shape}")

"""---

## ðŸ”ª Part 6: Data Splitting & Feature Scaling

**Goal:** To prepare the final dataset for training and testing a model.

### 1. Train-Test Split

**Explain:** Why is it essential to split your data into a training set and a testing set?

**Your Answer Here:**

*Type your explanation here. What would happen if we tested the model on the same data it was trained on? Would that give us a true measure of its performance?*

To make it easy for the model to memorize without noisy patterns.
If we test the model with the same data we trained it with this means the model will just memorize the answers without unerstaning.

**Task:** Define your target variable (`y`) as `NumTotalPurchases` and your features (`X`) as the rest of the columns. Use `train_test_split` to split your data into 85% for training and 15% for testing.
"""

from sklearn.model_selection import train_test_split

# TASK: Define X and y
x= df.drop(columns=['NumTotalPurchases'])
y= df['NumTotalPurchases']

# TASK: Split the data
x_train , x_test , y_train , y_test = train_test_split(x , y , test_size=0.15 , random_state=42)

print(f"X_train shape: {x_train.shape}")
print(f"X_test shape: {x_test.shape}")

"""### 2. Feature Scaling

**Explain:** What is the importance of feature scaling? What is the difference between `StandardScaler` and `MinMaxScaler`?

**Your Answer Here:**

*Type your explanation here. Think about how models might treat a feature with a range of 0-1 versus a feature with a range of 10,000-100,000.*

StandardScaler:
When data is roughly normally distributed

Algorithms that assume centered data (PCA, linear models)

When outliers are present (more robust than MinMax)

MinMaxScaler:
When you need bounded ranges (neural networks often prefer this)

When data doesn't follow normal distribution

When you want to preserve zero entries in sparse data

**Task:** Use `StandardScaler` to scale the numerical features in your data. **Important:** `fit` the scaler **only** on the training data and then `transform` both the training and testing data.
"""

from sklearn.preprocessing import StandardScaler

# TASK: Scale the data
scaler = StandardScaler()
numerical_cols = ['Income', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts',
                 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases',
                 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases',
                 'NumWebVisitsMonth', 'day_engaged', 'Age']

# Fit on training data
scaler.fit(x_train[numerical_cols])
# Transform both training and testing data

x_train_scaled = x_train.copy()
x_test_scaled = x_test.copy()

x_train_scaled[numerical_cols] = scaler.transform(x_train[numerical_cols])
x_test_scaled[numerical_cols] = scaler.transform(x_test[numerical_cols])
print("Scaled training data head:")
display(x_train.head())

"""**Question:** What is the risk if you `fit` the scaler on the entire dataset *before* splitting it into training and testing sets?

**Your Answer Here:**

*Type your answer here. This is a concept called 'data leakage'. What information from the test set would 'leak' into the training process if you did this?*

It could lead to leak in some information (distributions , outliers ...)
It could cause invalid evaluation.

---
"""